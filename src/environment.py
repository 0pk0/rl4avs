import gymnasium as gym
import highway_env
from highway_env.envs.roundabout_env import RoundaboutEnv
from gymnasium.envs.registration import register
import numpy as np


class CustomRoundaboutEnv(RoundaboutEnv):
    def __init__(self, config=None, render_mode=None):
        super().__init__(config, render_mode=render_mode)
        self._init_tracking_variables()
        
    def _init_tracking_variables(self):
        """Initialize tracking variables for reward system"""
        # Track agent progress and mistakes
        self.prev_position = None
        self.stationary_steps = 0
        self.total_distance_traveled = 0
        self.mistake_memory = []  # Track recent collision/failure patterns
        self.max_stationary_steps = 10  # Max allowed stationary steps before heavy penalty
        self.step_count = 0  # Track steps for efficiency bonus
        self.prev_speed = None # Added to track previous speed for comfort calculation
        
        # Course completion tracking
        self.initial_position = None
        self.has_entered_roundabout = False
        self.has_exited_roundabout = False
        self.roundabout_progress = 0
        self.completion_threshold = 150.0  # Distance needed to complete course
        self.exit_confirmation_threshold = 200.0  # Distance to confirm full exit
        self.max_distance_from_start = 0
        self.roundabout_center_distance = 0  # Track distance from roundabout center

    @classmethod
    def default_config(cls):
        config = super().default_config()
        config.update({
            "observation": {
                "type": "Kinematics",
                "vehicles_count": 15,
                "features": ["presence", "x", "y", "vx", "vy", "cos_h", "sin_h"],
                "normalize": True,
                "absolute": False,
                "order": "sorted",
            },
            "action": {
                "type": "DiscreteMetaAction",
                "longitudinal": True,
                "lateral": True,
                "target_speeds": [ 5, 10, 15, 20, 25, 30]
            },
            "simulation_frequency": 15,
            "policy_frequency": 1,
            "duration": 200,  # Much longer episodes for course completion
            "collision_reward": -8.0,         # Strong but learnable collision penalty
            "idle_penalty": -0.3,             # Strong idle penalty but not overwhelming
            "stationary_penalty": -1.0,       # Heavy penalty for staying still
            "progress_reward": 0.2,           # Reward for making progress
            "high_speed_reward": 0.4,         # Moderate speed reward
            "arrived_reward": 15.0,           # Very high completion reward!
            "lane_change_reward": 0.15,       # Moderate lane change reward
            "efficiency_bonus": 3.0,          # Higher bonus for fast completion
            "repeated_mistake_penalty": -3.0,  # Moderate penalty for repeated mistakes
            "reward_speed_range": [7.0, 9.0],
            "normalize_reward": False,
            "vehicles_count": 12,
            "vehicle_width": 2.0, # Added for near-miss calculation
        })
        return config

    def step(self, action):
        """Enhanced step method with aggressive reward shaping for course completion"""
        # Call the parent's step method first
        obs, reward, done, truncated, info = super().step(action)
        self.step_count += 1  # Track steps for efficiency bonus

        # Retrieve ego vehicle from environment
        ego_vehicle = self.vehicle
        
        if ego_vehicle is not None:
            current_position = (ego_vehicle.position[0], ego_vehicle.position[1])
            current_speed = ego_vehicle.speed
            # current_acceleration = np.linalg.norm(ego_vehicle.action.get("acceleration", [0, 0])) # Get acceleration

            # Calculate acceleration for comfort score
            current_acceleration = 0.0
            if self.prev_speed is not None:
                current_acceleration = abs(current_speed - self.prev_speed) / (1 / self.config["policy_frequency"])
            self.prev_speed = current_speed


            # Initialize starting position on first step
            if self.initial_position is None:
                self.initial_position = current_position
                print(f"üèÅ Starting position: ({current_position[0]:.1f}, {current_position[1]:.1f})")
            
            # üéØ ENHANCED ROUNDABOUT EXIT DETECTION
            distance_from_start = abs(current_position[0] - self.initial_position[0]) + \
                                abs(current_position[1] - self.initial_position[1])
            
            # Track maximum distance traveled (progress indicator)
            if distance_from_start > self.max_distance_from_start:
                self.max_distance_from_start = distance_from_start
            
            # Estimate roundabout center distance (assuming roundabout is around initial position)
            # This helps detect when vehicle is moving away from roundabout
            roundabout_center = self.initial_position  # Approximate center
            center_distance = np.linalg.norm(np.array(current_position) - np.array(roundabout_center))
            self.roundabout_center_distance = center_distance
            
            # üîÑ ROUNDABOUT ENTRY DETECTION
            if not self.has_entered_roundabout and distance_from_start > 50.0:
                self.has_entered_roundabout = True
                print(f"üîÑ ENTERED ROUNDABOUT at step {self.step_count}")
                print(f"   Entry position: ({current_position[0]:.1f}, {current_position[1]:.1f})")
                print(f"   Entry speed: {current_speed:.1f}")
            
            # üö™ ROUNDABOUT EXIT DETECTION 
            # More sophisticated detection: vehicle has traveled far AND is moving away from roundabout
            if (self.has_entered_roundabout and 
                not self.has_exited_roundabout and
                distance_from_start > self.completion_threshold and
                center_distance > self.completion_threshold * 0.8 and  # Moving away from center
                self.step_count > 30):  # Minimum steps for legitimate exit
                
                # Confirm vehicle is actively moving (not crashed/stuck)
                if current_speed > 5.0:
                    self.has_exited_roundabout = True
                    print(f"üö™ ROUNDABOUT EXIT DETECTED at step {self.step_count}")
                    print(f"   Exit position: ({current_position[0]:.1f}, {current_position[1]:.1f})")
                    print(f"   Exit speed: {current_speed:.1f}")
                    print(f"   Distance from start: {distance_from_start:.1f}")
                    print(f"   Distance from roundabout center: {center_distance:.1f}")
            
            # üèÜ FINAL COMPLETION DETECTION: Vehicle has fully exited and traveled sufficient distance
            if (self.has_exited_roundabout and 
                distance_from_start > self.exit_confirmation_threshold and
                current_speed > 3.0):  # Still moving after exit
                
                print(f"")
                print(f"üéâ =============================================")
                print(f"üéâ    ROUNDABOUT COURSE COMPLETED!          ")
                print(f"üéâ =============================================")
                print(f"üìä COMPLETION SUMMARY:")
                print(f"   ‚úÖ Total distance traveled: {distance_from_start:.1f} units")
                print(f"   ‚è±Ô∏è  Total steps taken: {self.step_count}")
                print(f"   üèÅ Final position: ({current_position[0]:.1f}, {current_position[1]:.1f})")
                print(f"   üöó Final speed: {current_speed:.1f} km/h")
                print(f"   üîÑ Roundabout entry: Step {3}")
                print(f"   üö™ Roundabout exit: Step ~{self.step_count-20}")
                print(f"   üèÜ Course completion: Step {self.step_count}")
                print(f"üéâ =============================================")
                print(f"")
                
                # Set arrival flag and force immediate termination
                info['arrived'] = True
                done = True  # Force episode termination immediately
                
                # Log this like a collision for debugging
                print(f"üí´ EPISODE TERMINATED: Course completion detected")
                print(f"   Termination reason: Successful roundabout navigation")
            
            # --- NEW METRICS CALCULATION ---
            # 1. Near-miss detection
            info['near_miss'] = False
            near_miss_threshold_distance = self.config.get("vehicle_width", 2.0) + 0.5 # Vehicle width + buffer
            for other in self.road.vehicles:
                if other is not ego_vehicle:
                    distance = np.linalg.norm(ego_vehicle.position - other.position)
                    # Check for near-miss: close but not colliding
                    if distance > self.config.get("vehicle_width", 2.0) / 2 and distance < near_miss_threshold_distance and not info.get('crashed', False):
                        info['near_miss'] = True
                        break
            
            # 2. Safety margin analysis (closest vehicle distance)
            min_distance_to_other = np.inf
            for other in self.road.vehicles:
                if other is not ego_vehicle:
                    distance = np.linalg.norm(ego_vehicle.position - other.position)
                    if distance < min_distance_to_other:
                        min_distance_to_other = distance
            info['safety_margin'] = min_distance_to_other # Store min distance for safety margin analysis
            
            # 3. Comfort score (penalize high acceleration/deceleration)
            # A simple comfort metric: inverse of acceleration magnitude
            # Higher acceleration -> lower comfort. Add a small constant to avoid division by zero.
            comfort = 1.0 / (current_acceleration + 0.5) if current_acceleration > 0.0 else 2.0 # Max comfort if no acceleration
            info['comfort_score'] = comfort

            # üö® AGGRESSIVE IDLE/STATIONARY PENALTIES
            if current_speed < 0.5:  # Very low speed threshold
                # Progressive idle penalty - gets worse over time
                idle_penalty = self.config.get("idle_penalty", -0.5)
                reward += idle_penalty
                self.stationary_steps += 1
                
                # HEAVY penalty for staying still too long
                if self.stationary_steps > self.max_stationary_steps:
                    stationary_penalty = self.config.get("stationary_penalty", -2.0)
                    # Exponentially increasing penalty
                    multiplier = min(self.stationary_steps - self.max_stationary_steps, 10)
                    reward += stationary_penalty * multiplier
                    
                    if self.stationary_steps > 20:  # Force action after 20 steps of being still
                        reward -= 10.0  # Massive penalty
                        
            else:
                self.stationary_steps = 0  # Reset counter when moving
                
            # üéØ PROGRESS REWARDS - Encourage forward movement
            if self.prev_position is not None:
                distance_moved = np.linalg.norm(np.array(current_position) - np.array(self.prev_position))
                self.total_distance_traveled += distance_moved
                
                if distance_moved > 0.1:  # Reward any meaningful movement
                    progress_reward = self.config.get("progress_reward", 0.3)
                    reward += progress_reward * distance_moved
            
            self.prev_position = current_position
            
            # üèÜ MASSIVE COMPLETION REWARD
            if info.get('arrived', False):
                arrival_reward = self.config.get("arrived_reward", 15.0)
                reward += arrival_reward
                
                # üöÄ EFFICIENCY BONUS - Extra reward for fast completion
                if self.step_count < 150:  # Reasonable completion time
                    efficiency_bonus = self.config.get("efficiency_bonus", 3.0)
                    speed_multiplier = max(1.0, (150 - self.step_count) / 100)
                    reward += efficiency_bonus * speed_multiplier
                    print(f"   üöÄ Efficiency bonus: +{efficiency_bonus * speed_multiplier:.2f}")
                
                # üèÉ DISTANCE BONUS - Reward for traveling sufficient distance
                distance_bonus = min(5.0, self.max_distance_from_start / 50.0)
                reward += distance_bonus
                print(f"   üèÉ Distance bonus: +{distance_bonus:.2f}")
                
                print(f"üéâ COURSE COMPLETED! Total reward this episode: {reward:.2f}")
                print(f"   üèÜ This should be the MAXIMUM reward possible!")
            
            # üí• ENHANCED COLLISION PENALTY with mistake tracking
            if info.get('crashed', False):
                collision_penalty = self.config.get("collision_reward", -15.0)
                reward += collision_penalty
                
                # Track mistake patterns to prevent repetition
                mistake_info = {
                    'position': current_position,
                    'speed': current_speed,
                    'action': action
                }
                self.mistake_memory.append(mistake_info)
                
                # Additional penalty for repeated mistakes at similar locations
                if len(self.mistake_memory) > 1:
                    for prev_mistake in self.mistake_memory[-3:]:  # Check last 3 mistakes
                        pos_diff = np.linalg.norm(np.array(current_position) - np.array(prev_mistake['position']))
                        if pos_diff < 2.0:  # Similar location
                            repeated_penalty = self.config.get("repeated_mistake_penalty", -5.0)
                            reward += repeated_penalty
                            break
                
                print(f"")
                print(f"üí• =============================================")
                print(f"üí•    COLLISION DETECTED!                   ")
                print(f"üí• =============================================")
                print(f"üìä COLLISION SUMMARY:")
                print(f"   üí• Collision position: ({current_position[0]:.1f}, {current_position[1]:.1f})")
                print(f"   ‚è±Ô∏è  Steps before collision: {self.step_count}")
                print(f"   üöó Speed at impact: {current_speed:.1f} km/h")
                print(f"   üîÑ Had entered roundabout: {self.has_entered_roundabout}")
                print(f"   üö™ Had exited roundabout: {self.has_exited_roundabout}")
                print(f"   üìè Distance from start: {np.linalg.norm(np.array(current_position) - np.array(self.initial_position)):.1f}")
                print(f"   üéØ Penalty applied: {collision_penalty:.1f}")
                print(f"   üìä Episode reward: {reward:.2f}")
                print(f"üí• =============================================")
                print(f"")
                print(f"üí´ EPISODE TERMINATED: Collision detected")
                print(f"   Termination reason: Vehicle collision")
            
            # üöó SPEED REWARDS - Encourage appropriate speed
            if 8.0 <= current_speed <= 15.0:  # Optimal speed range for roundabouts
                speed_reward = self.config.get("high_speed_reward", 0.6)
                reward += speed_reward
            elif current_speed > 20.0:  # Penalize excessive speed
                reward -= 0.3
            
            # üîÑ LANE CHANGE REWARDS (enhanced)
            if action in [0, 2]:  # LANE_LEFT or LANE_RIGHT
                lane_reward = self.config.get("lane_change_reward", 0.2)
                reward += lane_reward
            
            # üö´ DISCOURAGE IDLE ACTION
            if action == 1:  # IDLE action
                reward -= 0.2  # Small penalty for choosing to do nothing
            
            # üèÉ ENCOURAGE SPEED ACTIONS
            if action in [3]:  # FASTER action
                reward += 0.1  # Small reward for acceleration
        
        return obs, reward, done, truncated, info
    
    def reset(self, **kwargs):
        """Reset tracking variables"""
        obs, info = super().reset(**kwargs)
        # Ensure tracking variables are initialized
        if not hasattr(self, 'mistake_memory'):
            self._init_tracking_variables()
        else:
            self.prev_position = None
            self.stationary_steps = 0
            self.total_distance_traveled = 0
            self.mistake_memory = self.mistake_memory[-5:] if len(self.mistake_memory) > 5 else []  # Keep recent mistakes
            self.step_count = 0
            # Reset completion tracking
            self.initial_position = None
            self.has_entered_roundabout = False
            self.has_exited_roundabout = False
            self.roundabout_progress = 0
            self.max_distance_from_start = 0
            self.roundabout_center_distance = 0
            self.prev_speed = None # Reset previous speed
        return obs, info

def register_custom_env():
    """Register the custom environment"""
    try:
        register(
            id='custom-roundabout-v0',
            entry_point='src.environment:CustomRoundaboutEnv',
        )
        print("Custom environment registered successfully!")
    except Exception as e:
        # This is expected on subsequent calls
        if "already in registry" not in str(e):
            print(f"Environment registration error: {e}")


def make_env(env_name="roundabout-v0", custom=False):
    """Create environment with optional customization"""
    if custom:
        register_custom_env()
        return gym.make('custom-roundabout-v0')
    else:
        return gym.make(env_name)
